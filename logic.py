import sqlite3
import google.generativeai as genai
import os
from dotenv import load_dotenv # Ù…ÙƒØªØ¨Ø© Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ù€ .env

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù .env
load_dotenv()

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Gemini API ---
# âš ï¸ Ù…ØªÙ†Ø³Ø§Ø´ ØªØ­Ø· Ù…ÙØªØ§Ø­Ùƒ Ù‡Ù†Ø§
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GROQ_API_KEY = os.getenv("Groq_API_KEY")
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø© (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©)
genai.configure(api_key=GOOGLE_API_KEY)

def get_db_connection():
    conn = sqlite3.connect('aroh_ezay.db')
    conn.row_factory = sqlite3.Row
    return conn

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„ÙƒØ§Ø´ (Ø§Ù„Ø°Ø§ÙƒØ±Ø©) ---
def get_cached_ai_response(from_loc, to_loc):
    """Ø§Ù„ØªØ¯ÙˆÙŠÙ€Ø± ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø¹Ù† Ø±Ø¯ Ø³Ø§Ø¨Ù‚"""
    conn = get_db_connection()
    query = "SELECT response_text FROM ai_routes_cache WHERE from_loc = ? AND to_loc = ?"
    result = conn.execute(query, (from_loc, to_loc)).fetchone()
    conn.close()
    return result['response_text'] if result else None

def save_ai_response_to_cache(from_loc, to_loc, text):
    """Ø­ÙØ¸ Ø±Ø¯ Ø§Ù„Ù€ AI ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„"""
    conn = get_db_connection()
    query = "INSERT INTO ai_routes_cache (from_loc, to_loc, response_text) VALUES (?, ?, ?)"
    conn.execute(query, (from_loc, to_loc, text))
    conn.commit()
    conn.close()

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù€ AI ---

import google.generativeai as genai
from groq import Groq

def get_ai_advice(from_loc, to_loc):
    # 1. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
    prompt = f"""
    Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ÙˆØ§ØµÙ„Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©. Ù…Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ³Ø£Ù„ Ø¥Ø²Ø§ÙŠ ÙŠØ±ÙˆØ­ Ù…Ù† {from_loc} Ù„Ù€ {to_loc}.
    Ø¬Ø§ÙˆØ¨ Ø¨Ù„Ù‡Ø¬Ø© Ù…ØµØ±ÙŠØ© Ø¹Ø§Ù…ÙŠØ© Ø¨Ø³ÙŠØ·Ø©. Ù†Ø¸Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù†Ù‚Ø·.
    Ù‚ÙˆÙ„Ù‡ ÙŠØ±ÙƒØ¨ Ø¥ÙŠÙ‡ ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„ÙˆÙ‚Øª Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ.
    Ù†Ø¨Ù‡ Ø¯Ø§ÙŠÙ…Ø§ Ø¹Ù„ÙŠÙ‡ Ø§Ù† Ø§Ù„Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ù„ÙŠ Ø¨ØªØ¯ÙŠÙ‡Ø§Ù„Ù‡ Ù‡ÙŠ Ø§Ø³Ø¹Ø§Ø± ØªÙ‚Ø±ÙŠØ¨ÙŠÙ‡ Ù…Ø´ Ø¨Ø§Ù„Ø¸Ø¨Ø· Ø¹Ø´Ø§Ù† Ø¯Ø§ÙŠÙ…Ø§ Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª ÙÙŠ ØªØºÙŠØ±.
    Ù„Ùˆ Ù…Ø´ Ø¹Ø§Ø±Ù Ø§Ù„Ø·Ø±ÙŠÙ‚ØŒ Ù‚ÙˆÙ„Ù‡ ÙŠØ±ÙˆØ­ Ù„Ø£Ù‚Ø±Ø¨ Ù…Ø­Ø·Ø© Ù…ØªØ±Ùˆ ÙˆÙŠØ³Ø£Ù„ Ù‡Ù†Ø§Ùƒ.
    Ø§ÙƒØªØ¨ Ø§Ù„Ø±Ø¯ ÙƒÙ†Øµ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø©.
    """

    # 2. Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù…Ù† Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ø£Ø®ÙŠØ±)
    # Ø¶ÙŠÙ Ù‡Ù†Ø§ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ø¬Ø¯ÙŠØ¯ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ù‡ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø¨Ø³Ù‡ÙˆÙ„Ø©
    gemini_models_priority = [
        'gemini-3-flash-preview',                # Ø£Ø­Ø¯Ø« ÙˆØ£Ø°ÙƒÙ‰ (ØªØ¬Ø±ÙŠØ¨ÙŠ)
        'gemini-2.5-flash',                      # Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø§Ù„Ù‚ÙˆÙŠ
        'gemini-2.5-flash-preview-09-2025',      # Ø¨Ø¯ÙŠÙ„ Ø£ÙˆÙ„
        'gemini-2.5-flash-lite-preview-09-2025'  # Ø¨Ø¯ÙŠÙ„ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹
    ]

    # 3. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Gemini Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
    for model_name in gemini_models_priority:
        try:
            # print(f"Trying Gemini Model: {model_name}...") # (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ù„Ù„Ø¯ÙŠÙŠØ¨Ø§Ø¬ Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù Ù‡Ùˆ Ø´ØºØ§Ù„ Ø¨Ù€ Ù…ÙŠÙ†
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            
            # Ù„Ùˆ Ù†Ø¬Ø­ ÙˆØ±Ø¬Ø¹ Ù†ØµØŒ Ù†Ø±Ø¬Ø¹Ù‡ ÙˆÙ†Ø®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© ÙÙˆØ±Ø§Ù‹
            if response.text:
                return response.text
                
        except Exception as e:
            # Ù„Ùˆ ÙØ´Ù„ØŒ Ø¨Ù†Ø·Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ ÙˆÙ†ÙƒÙ…Ù„ Ø§Ù„Ù„ÙØ© Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡ (continue)
            print(f"âš ï¸ Failed with {model_name}: {e}")
            continue 

    # 4. Ù„Ùˆ Ø§Ù„Ù„ÙˆØ¨ Ø®Ù„ØµØª ÙˆÙ…ÙÙŠØ´ ÙˆÙ„Ø§ Ù…ÙˆØ¯ÙŠÙ„ Gemini Ø§Ø´ØªØºÙ„ØŒ Ù†Ø±ÙˆØ­ Ù„Ù€ Groq (Ø§Ù„Ù…Ù„Ø§Ø° Ø§Ù„Ø£Ø®ÙŠØ±)
    try:
        print("ğŸ”» All Gemini models failed. Switching to Groq...")
        groq_client = Groq(api_key= GROQ_API_KEY) # ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù…ÙˆØ¬ÙˆØ¯
        completion = groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
        )
        return completion.choices[0].message.content
        
    except Exception as groq_error:
        print(f"âŒ Groq also failed: {groq_error}")
        return "âš ï¸ Ù…Ø¹Ù„Ø´ØŒ Ø§Ù„Ø³ÙŠØ³ØªÙ… Ø¹Ù„ÙŠÙ‡ Ø¶ØºØ· ÙƒØ¨ÙŠØ± Ø­Ø§Ù„ÙŠØ§Ù‹ ÙˆÙ…Ø´ Ù‚Ø§Ø¯Ø±ÙŠÙ† Ù†ÙˆØµÙ„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¯Ù„ÙˆÙ‚ØªÙŠ. Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠ ÙƒÙ…Ø§Ù† Ø¯Ù‚ÙŠÙ‚Ø©."




# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„Ø¹Ø±Ø¶ ---
def clean_text(text):
    if not text: return ""
    return text.replace("Ù…Ø­Ø·Ø© ", "").replace("Ø§ØªØ¬Ø§Ù‡ ", "")

def humanize_step(step):
    t_type = step['transport_type']
    line = step['line_name']
    boarding = step['boarding_point']
    exit_p = step['exit_point']
    direction = step['direction_details']
    tip = step['human_tip']

    if t_type == 'Ù…ØªØ±Ùˆ':
        msg = f"Ù‡ØªØ±ÙƒØ¨ Ø§Ù„Ù…ØªØ±Ùˆ Ù…Ù† Ù…Ø­Ø·Ø© {clean_text(boarding)} ({line}) ÙÙŠ Ø§ØªØ¬Ø§Ù‡ {clean_text(direction)}ØŒ ÙˆÙ‡ØªÙ†Ø²Ù‘Ù„ ÙÙŠ Ù…Ø­Ø·Ø© {clean_text(exit_p)}."
    elif t_type == 'Ù…ÙŠÙƒØ±ÙˆØ¨Ø§Øµ':
        loc_desc = f"ÙˆØ¯Ù‡ Ù‡ØªÙ„Ø§Ù‚ÙŠÙ‡ Ø¨ÙŠØ­Ù…Ù‘Ù„ Ù…Ù† {boarding}" if boarding else "Ø§Ø³Ø£Ù„ Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ø¹Ù…ÙˆÙ…ÙŠ"
        msg = f"Ù‡ØªØ±ÙƒØ¨ Ù…ÙŠÙƒØ±ÙˆØ¨Ø§Øµ {line}ØŒ {loc_desc}ØŒ ÙˆÙ‡ØªÙ†Ø²Ù‘Ù„ Ø¹Ù†Ø¯ {exit_p}."
    else:
        msg = f"Ø§Ø±ÙƒØ¨ {t_type} ({line}) Ù…Ù† {boarding} ÙˆØ§Ù†Ø²Ù„ ÙÙŠ {exit_p}."

    if tip: msg += f" (Ù†ØµÙŠØ­Ø©: {tip})"
    return msg

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (The Brain) ---
def search_routes_logic(from_area, to_area):
    conn = get_db_connection()
    
    # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§ Ø¨ÙŠØ² Ø§Ù„Ø£ØµÙ„ÙŠØ© (Structured Data)
    query = """
        SELECT r.* FROM routes r
        JOIN locations l1 ON r.from_location_id = l1.id
        JOIN locations l2 ON r.to_location_id = l2.id
        WHERE l1.name LIKE ? AND l2.name LIKE ?
    """
    db_routes = conn.execute(query, (f'%{from_area}%', f'%{to_area}%')).fetchall()
    conn.close()
    
    results = []
    
    # Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ Ø¯Ø§ØªØ§ Ù…Ù†Ø¸Ù…Ø©ØŒ Ù†Ø¹Ø±Ø¶Ù‡Ø§
    if db_routes:
        for route in db_routes:
            # ÙØªØ­ Ø§ØªØµØ§Ù„ Ø¬Ø¯ÙŠØ¯ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø®Ø·ÙˆØ§Øª
            conn_steps = get_db_connection()
            steps_query = "SELECT * FROM route_steps WHERE route_id = ? ORDER BY step_order"
            steps = conn_steps.execute(steps_query, (route['id'],)).fetchall()
            conn_steps.close()
            
            results.append({
                "type": "db",
                "total_price": route['total_price'],
                "total_time": route['total_time'],
                "tag": route['route_tag'],
                "steps": [humanize_step(s) for s in steps]
            })
        return results

    # 2. Ù„Ùˆ Ù…ÙÙŠØ´ Ø¯Ø§ØªØ§ Ù…Ù†Ø¸Ù…Ø© -> Ù†Ø´ÙˆÙ "Ø§Ù„ÙƒØ§Ø´" (Ù‡Ù„ Ø­Ø¯ Ø³Ø£Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¯Ù‡ Ù‚Ø¨Ù„ ÙƒØ¯Ù‡ØŸ)
    cached_response = get_cached_ai_response(from_area, to_area)
    if cached_response:
        # ÙŠØ§ Ø³Ù„Ø§Ù…! Ù„Ù‚ÙŠÙ†Ø§Ù‡ Ù…ØªØ®Ø²Ù†ØŒ Ù†Ø±Ø¬Ø¹Ù‡ Ø¹Ù„Ø·ÙˆÙ„ Ù…Ù† ØºÙŠØ± Ù…Ø§ Ù†ÙƒÙ„Ù… Ø¬ÙˆØ¬Ù„
        return [{"type": "ai", "content": cached_response, "source": "cache"}]

    # 3. Ù„Ùˆ Ù…Ø´ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ -> Ù†ÙƒÙ„Ù… Gemini (Live Request)
    ai_msg = get_ai_advice(from_area, to_area)
    
    if ai_msg:
        # Ù†Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ø¹Ø´Ø§Ù† Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø¬Ø§ÙŠØ©
        save_ai_response_to_cache(from_area, to_area, ai_msg)
        return [{"type": "ai", "content": ai_msg, "source": "live"}]
    else:
        # Ù„Ùˆ Ø­ØªÙ‰ Gemini Ù…Ø±Ø¯Ø´ (Ù†Øª Ù‚Ø§Ø·Ø¹ Ø£Ùˆ Ø®Ø·Ø£)
        return [{"type": "ai", "content": "Ù…Ø¹Ù„Ø´ Ø§Ù„Ø³ÙŠØ³ØªÙ… ÙˆØ§Ù‚Ø¹ØŒ Ø§Ø³Ø£Ù„ Ø£Ù‚Ø±Ø¨ Ø³ÙˆØ§Ù‚."}]

def get_all_areas_logic(search_term):
    conn = get_db_connection()
    query = "SELECT name FROM locations WHERE name LIKE ?"
    areas = conn.execute(query, (f'%{search_term}%',)).fetchall()
    conn.close()
    return [a['name'] for a in areas]


